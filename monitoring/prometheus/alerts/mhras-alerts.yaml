# Alerting rules for MHRAS

groups:
  - name: mhras_critical
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          rate(mhras_errors_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # High response latency
      - alert: HighResponseLatency
        expr: |
          histogram_quantile(0.95, rate(mhras_http_request_duration_seconds_bucket[5m])) > 10
        for: 5m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "High API response latency"
          description: "95th percentile latency is {{ $value }}s (threshold: 10s)"

      # Service down
      - alert: ServiceDown
        expr: up{job="mhras-api"} == 0
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "MHRAS API is down"
          description: "Service {{ $labels.instance }} has been down for more than 2 minutes"

  - name: mhras_warning
    interval: 30s
    rules:
      # Feature drift detected
      - alert: FeatureDriftDetected
        expr: |
          mhras_drift_score > 0.3
        for: 10m
        labels:
          severity: warning
          component: ml
        annotations:
          summary: "Feature drift detected"
          description: "Drift score for {{ $labels.feature }} is {{ $value }} (threshold: 0.3)"

      # Large human review queue
      - alert: LargeHumanReviewQueue
        expr: |
          mhras_human_review_queue_size > 50
        for: 15m
        labels:
          severity: warning
          component: governance
        annotations:
          summary: "Human review queue is large"
          description: "Queue size is {{ $value }} cases (threshold: 50)"

      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          container_memory_usage_bytes{pod=~"mhras-api-.*"} / container_spec_memory_limit_bytes{pod=~"mhras-api-.*"} > 0.85
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.pod }}"

      # High CPU usage
      - alert: HighCPUUsage
        expr: |
          rate(container_cpu_usage_seconds_total{pod=~"mhras-api-.*"}[5m]) > 1.7
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }} cores on {{ $labels.pod }}"

      # Slow model inference
      - alert: SlowModelInference
        expr: |
          histogram_quantile(0.95, rate(mhras_model_inference_duration_seconds_bucket[5m])) > 2
        for: 10m
        labels:
          severity: warning
          component: ml
        annotations:
          summary: "Slow model inference"
          description: "95th percentile inference time is {{ $value }}s (threshold: 2s)"

  - name: mhras_info
    interval: 60s
    rules:
      # High critical risk screenings
      - alert: HighCriticalRiskRate
        expr: |
          rate(mhras_screenings_total{risk_level="critical"}[1h]) > 0.1
        for: 30m
        labels:
          severity: info
          component: clinical
        annotations:
          summary: "High rate of critical risk screenings"
          description: "Critical risk rate is {{ $value | humanizePercentage }} over the last hour"

      # Model performance degradation
      - alert: ModelPerformanceDegradation
        expr: |
          rate(mhras_screenings_total[1h]) < 0.5 * rate(mhras_screenings_total[1h] offset 24h)
        for: 1h
        labels:
          severity: info
          component: ml
        annotations:
          summary: "Screening rate has dropped significantly"
          description: "Current rate is 50% lower than 24 hours ago"
