{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking Example\n",
    "\n",
    "This notebook demonstrates how to use the Experiment Tracker to log, track, and compare machine learning experiments in the Mental Health Risk Assessment System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from src.ds.experiment_tracker import ExperimentTracker\n",
    "from src.ds.storage import FileSystemStorage\n",
    "from src.database.connection import get_db_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Experiment Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage and database\n",
    "storage = FileSystemStorage(base_path=\"../experiments/artifacts\")\n",
    "db = get_db_connection()\n",
    "\n",
    "# Create experiment tracker\n",
    "tracker = ExperimentTracker(storage_backend=storage, db_connection=db)\n",
    "\n",
    "print(\"✓ Experiment tracker initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic patient assessment data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'phq9_score': np.random.randint(0, 27, n_samples),\n",
    "    'gad7_score': np.random.randint(0, 21, n_samples),\n",
    "    'pcl5_score': np.random.randint(0, 80, n_samples),\n",
    "    'sleep_hours': np.random.uniform(3, 10, n_samples),\n",
    "    'previous_episodes': np.random.randint(0, 5, n_samples)\n",
    "})\n",
    "\n",
    "# Create target variable (high risk if scores are elevated)\n",
    "data['high_risk'] = (\n",
    "    (data['phq9_score'] > 15) | \n",
    "    (data['gad7_score'] > 10) | \n",
    "    (data['pcl5_score'] > 40)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"High risk cases: {data['high_risk'].sum()} ({data['high_risk'].mean()*100:.1f}%)\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new run\n",
    "run = tracker.start_run(\n",
    "    experiment_name=\"mental_health_risk_prediction\",\n",
    "    run_name=\"baseline_random_forest\",\n",
    "    tags={\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"purpose\": \"baseline\",\n",
    "        \"dataset\": \"synthetic_v1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Started run: {run.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = data.drop('high_risk', axis=1)\n",
    "y = data['high_risk']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Log dataset parameters\n",
    "tracker.log_params({\n",
    "    \"n_samples\": len(data),\n",
    "    \"n_features\": X.shape[1],\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 10,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "# Log hyperparameters\n",
    "tracker.log_params(model_params)\n",
    "\n",
    "# Train\n",
    "model = RandomForestClassifier(**model_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"✓ Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and log metrics\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"f1_score\": f1_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_proba)\n",
    "}\n",
    "\n",
    "tracker.log_metrics(metrics)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and log model artifact\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "model_path = \"../models/baseline_rf.pkl\"\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "tracker.log_artifact(model_path, artifact_type=\"model\")\n",
    "print(f\"✓ Model saved and logged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and log feature importance plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance - Baseline Random Forest')\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = \"../plots/baseline_feature_importance.png\"\n",
    "os.makedirs(\"../plots\", exist_ok=True)\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "tracker.log_artifact(plot_path, artifact_type=\"plot\")\n",
    "print(\"✓ Feature importance plot logged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the run\n",
    "tracker.end_run(status=\"FINISHED\")\n",
    "print(\"✓ Run completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Tuned Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start new run with different hyperparameters\n",
    "run2 = tracker.start_run(\n",
    "    experiment_name=\"mental_health_risk_prediction\",\n",
    "    run_name=\"tuned_random_forest\",\n",
    "    tags={\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"purpose\": \"hyperparameter_tuning\",\n",
    "        \"dataset\": \"synthetic_v1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log dataset parameters\n",
    "tracker.log_params({\n",
    "    \"n_samples\": len(data),\n",
    "    \"n_features\": X.shape[1],\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42\n",
    "})\n",
    "\n",
    "# Tuned hyperparameters\n",
    "tuned_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 15,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "tracker.log_params(tuned_params)\n",
    "\n",
    "# Train\n",
    "model2 = RandomForestClassifier(**tuned_params)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred2 = model2.predict(X_test)\n",
    "y_pred_proba2 = model2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics2 = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred2),\n",
    "    \"f1_score\": f1_score(y_test, y_pred2),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_proba2)\n",
    "}\n",
    "\n",
    "tracker.log_metrics(metrics2)\n",
    "\n",
    "print(\"Tuned Model Performance:\")\n",
    "for metric, value in metrics2.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_path2 = \"../models/tuned_rf.pkl\"\n",
    "joblib.dump(model2, model_path2)\n",
    "tracker.log_artifact(model_path2, artifact_type=\"model\")\n",
    "\n",
    "tracker.end_run(status=\"FINISHED\")\n",
    "print(\"\\n✓ Tuned model run completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search all runs in the experiment\n",
    "runs = tracker.search_runs(\n",
    "    experiment_name=\"mental_health_risk_prediction\",\n",
    "    order_by=[\"metrics.roc_auc DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(runs)} runs\\n\")\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"Run: {run.run_name}\")\n",
    "    print(f\"  ID: {run.run_id}\")\n",
    "    print(f\"  Status: {run.status}\")\n",
    "    print(f\"  Metrics:\")\n",
    "    for metric, values in run.metrics.items():\n",
    "        # Get the latest value\n",
    "        latest_value = values[-1][0] if values else None\n",
    "        print(f\"    {metric}: {latest_value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare specific runs\n",
    "comparison = tracker.compare_runs(\n",
    "    run_ids=[run.run_id, run2.run_id],\n",
    "    metric_names=[\"accuracy\", \"f1_score\", \"roc_auc\"]\n",
    ")\n",
    "\n",
    "print(\"Run Comparison:\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Use Logged Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best run\n",
    "best_run = runs[0]  # Already sorted by ROC AUC\n",
    "\n",
    "print(f\"Best run: {best_run.run_name}\")\n",
    "print(f\"ROC AUC: {best_run.metrics['roc_auc'][-1][0]:.4f}\")\n",
    "\n",
    "# List artifacts\n",
    "print(f\"\\nArtifacts ({len(best_run.artifacts)}):\")\n",
    "for artifact in best_run.artifacts:\n",
    "    print(f\"  - {artifact.artifact_type}: {artifact.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Initializing** the experiment tracker\n",
    "2. **Starting runs** with metadata and tags\n",
    "3. **Logging parameters** (hyperparameters and dataset info)\n",
    "4. **Logging metrics** (accuracy, F1, ROC AUC)\n",
    "5. **Logging artifacts** (models and plots)\n",
    "6. **Searching and comparing** runs\n",
    "7. **Retrieving** the best model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Integrate with hyperparameter optimization\n",
    "- Link experiments to model registry\n",
    "- Set up automated experiment tracking in production pipelines\n",
    "- Use experiment tracking for A/B testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
