{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization Example\n",
    "\n",
    "This notebook demonstrates how to use the Hyperparameter Optimizer with experiment tracking to systematically tune model parameters for the Mental Health Risk Assessment System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "from src.ds.hyperparameter_optimizer import HyperparameterOptimizer\n",
    "from src.ds.experiment_tracker import ExperimentTracker\n",
    "from src.ds.storage import FileSystemStorage\n",
    "from src.database.connection import get_db_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment tracker\n",
    "storage = FileSystemStorage(base_path=\"../experiments/artifacts\")\n",
    "db = get_db_connection()\n",
    "tracker = ExperimentTracker(storage_backend=storage, db_connection=db)\n",
    "\n",
    "# Initialize hyperparameter optimizer\n",
    "optimizer = HyperparameterOptimizer(\n",
    "    experiment_tracker=tracker,\n",
    "    strategy=\"bayesian\"  # or \"grid\", \"random\"\n",
    ")\n",
    "\n",
    "print(\"✓ Components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic patient assessment data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'phq9_score': np.random.randint(0, 27, n_samples),\n",
    "    'gad7_score': np.random.randint(0, 21, n_samples),\n",
    "    'pcl5_score': np.random.randint(0, 80, n_samples),\n",
    "    'sleep_hours': np.random.uniform(3, 10, n_samples),\n",
    "    'previous_episodes': np.random.randint(0, 5, n_samples),\n",
    "    'social_support': np.random.randint(1, 10, n_samples)\n",
    "})\n",
    "\n",
    "# Create target (high risk if multiple elevated scores)\n",
    "data['high_risk'] = (\n",
    "    ((data['phq9_score'] > 15) & (data['gad7_score'] > 10)) |\n",
    "    (data['pcl5_score'] > 50) |\n",
    "    ((data['previous_episodes'] >= 3) & (data['social_support'] < 4))\n",
    ").astype(int)\n",
    "\n",
    "# Split data\n",
    "X = data.drop('high_risk', axis=1)\n",
    "y = data['high_risk']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Positive class ratio: {y_train.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Random Forest Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "def rf_objective(params):\n",
    "    \"\"\"Objective function for Random Forest optimization\"\"\"\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        max_depth=int(params['max_depth']),\n",
    "        min_samples_split=int(params['min_samples_split']),\n",
    "        min_samples_leaf=int(params['min_samples_leaf']),\n",
    "        max_features=params['max_features'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Use cross-validation for robust evaluation\n",
    "    scores = cross_val_score(\n",
    "        model, X_train, y_train,\n",
    "        cv=5, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Define parameter space\n",
    "rf_param_space = {\n",
    "    'n_estimators': (50, 300),\n",
    "    'max_depth': (5, 30),\n",
    "    'min_samples_split': (2, 20),\n",
    "    'min_samples_leaf': (1, 10),\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"Parameter space defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "print(\"Starting Random Forest optimization...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "rf_result = optimizer.optimize(\n",
    "    objective_function=rf_objective,\n",
    "    param_space=rf_param_space,\n",
    "    n_trials=50,\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Optimization complete!\")\n",
    "print(f\"\\nBest ROC AUC: {rf_result.best_score:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in rf_result.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Optimization history\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rf_result.optimization_history, marker='o')\n",
    "plt.axhline(y=rf_result.best_score, color='r', linestyle='--', label='Best Score')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.title('Optimization History - Random Forest')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter importance\n",
    "plt.subplot(1, 2, 2)\n",
    "importance_df = pd.DataFrame({\n",
    "    'parameter': list(rf_result.param_importance.keys()),\n",
    "    'importance': list(rf_result.param_importance.values())\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.barh(importance_df['parameter'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Parameter Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=int(rf_result.best_params['n_estimators']),\n",
    "    max_depth=int(rf_result.best_params['max_depth']),\n",
    "    min_samples_split=int(rf_result.best_params['min_samples_split']),\n",
    "    min_samples_leaf=int(rf_result.best_params['min_samples_leaf']),\n",
    "    max_features=rf_result.best_params['max_features'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  ROC AUC: {test_auc:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Gradient Boosting Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective for Gradient Boosting\n",
    "def gb_objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting optimization\"\"\"\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=int(params['max_depth']),\n",
    "        min_samples_split=int(params['min_samples_split']),\n",
    "        subsample=params['subsample'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(\n",
    "        model, X_train, y_train,\n",
    "        cv=5, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Define parameter space\n",
    "gb_param_space = {\n",
    "    'n_estimators': (50, 200),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'max_depth': (3, 10),\n",
    "    'min_samples_split': (2, 20),\n",
    "    'subsample': (0.6, 1.0)\n",
    "}\n",
    "\n",
    "print(\"Gradient Boosting parameter space defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "print(\"Starting Gradient Boosting optimization...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "gb_result = optimizer.optimize(\n",
    "    objective_function=gb_objective,\n",
    "    param_space=gb_param_space,\n",
    "    n_trials=50,\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Optimization complete!\")\n",
    "print(f\"\\nBest ROC AUC: {gb_result.best_score:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in gb_result.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GB optimization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gb_result.optimization_history, marker='o', color='green')\n",
    "plt.axhline(y=gb_result.best_score, color='r', linestyle='--', label='Best Score')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.title('Optimization History - Gradient Boosting')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "importance_df = pd.DataFrame({\n",
    "    'parameter': list(gb_result.param_importance.keys()),\n",
    "    'importance': list(gb_result.param_importance.values())\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.barh(importance_df['parameter'], importance_df['importance'], color='green')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Parameter Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best GB model\n",
    "best_gb = GradientBoostingClassifier(\n",
    "    n_estimators=int(gb_result.best_params['n_estimators']),\n",
    "    learning_rate=gb_result.best_params['learning_rate'],\n",
    "    max_depth=int(gb_result.best_params['max_depth']),\n",
    "    min_samples_split=int(gb_result.best_params['min_samples_split']),\n",
    "    subsample=gb_result.best_params['subsample'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "gb_pred_proba = best_gb.predict_proba(X_test)[:, 1]\n",
    "gb_pred = best_gb.predict(X_test)\n",
    "\n",
    "gb_test_auc = roc_auc_score(y_test, gb_pred_proba)\n",
    "gb_test_f1 = f1_score(y_test, gb_pred)\n",
    "\n",
    "# Compare results\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting'],\n",
    "    'CV ROC AUC': [rf_result.best_score, gb_result.best_score],\n",
    "    'Test ROC AUC': [test_auc, gb_test_auc],\n",
    "    'Test F1': [test_f1, gb_test_f1]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC AUC comparison\n",
    "models = comparison['Model']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison['CV ROC AUC'], width, label='CV ROC AUC', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison['Test ROC AUC'], width, label='Test ROC AUC', alpha=0.8)\n",
    "axes[0].set_ylabel('ROC AUC')\n",
    "axes[0].set_title('Model Performance Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[1].bar(models, comparison['Test F1'], color=['blue', 'green'], alpha=0.7)\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('Test F1 Score Comparison')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Optimization Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all trials for Random Forest\n",
    "trials_df = pd.DataFrame(rf_result.all_trials)\n",
    "\n",
    "print(f\"Total trials: {len(trials_df)}\")\n",
    "print(f\"\\nTop 5 trials:\")\n",
    "print(trials_df.nlargest(5, 'score'))\n",
    "\n",
    "print(f\"\\nBottom 5 trials:\")\n",
    "print(trials_df.nsmallest(5, 'score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze parameter relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# n_estimators vs score\n",
    "axes[0, 0].scatter(trials_df['n_estimators'], trials_df['score'], alpha=0.6)\n",
    "axes[0, 0].set_xlabel('n_estimators')\n",
    "axes[0, 0].set_ylabel('ROC AUC')\n",
    "axes[0, 0].set_title('n_estimators vs Performance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# max_depth vs score\n",
    "axes[0, 1].scatter(trials_df['max_depth'], trials_df['score'], alpha=0.6, color='orange')\n",
    "axes[0, 1].set_xlabel('max_depth')\n",
    "axes[0, 1].set_ylabel('ROC AUC')\n",
    "axes[0, 1].set_title('max_depth vs Performance')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# min_samples_split vs score\n",
    "axes[1, 0].scatter(trials_df['min_samples_split'], trials_df['score'], alpha=0.6, color='green')\n",
    "axes[1, 0].set_xlabel('min_samples_split')\n",
    "axes[1, 0].set_ylabel('ROC AUC')\n",
    "axes[1, 0].set_title('min_samples_split vs Performance')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# min_samples_leaf vs score\n",
    "axes[1, 1].scatter(trials_df['min_samples_leaf'], trials_df['score'], alpha=0.6, color='red')\n",
    "axes[1, 1].set_xlabel('min_samples_leaf')\n",
    "axes[1, 1].set_ylabel('ROC AUC')\n",
    "axes[1, 1].set_title('min_samples_leaf vs Performance')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimization visualization\n",
    "import os\n",
    "os.makedirs(\"../optimization_results\", exist_ok=True)\n",
    "\n",
    "viz_path = optimizer.visualize_optimization(\n",
    "    output_path=\"../optimization_results/rf_optimization.png\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Optimization visualization saved to: {viz_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best parameters\n",
    "import json\n",
    "\n",
    "best_params_path = \"../optimization_results/best_params.json\"\n",
    "with open(best_params_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'random_forest': rf_result.best_params,\n",
    "        'gradient_boosting': gb_result.best_params\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"✓ Best parameters saved to: {best_params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Setting up** hyperparameter optimization with experiment tracking\n",
    "2. **Defining** objective functions and parameter spaces\n",
    "3. **Running** Bayesian optimization for multiple models\n",
    "4. **Visualizing** optimization history and parameter importance\n",
    "5. **Comparing** optimized models\n",
    "6. **Analyzing** trial results and parameter relationships\n",
    "7. **Exporting** results for documentation\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Bayesian optimization efficiently explores parameter space\n",
    "- Parameter importance helps focus future tuning efforts\n",
    "- Cross-validation provides robust performance estimates\n",
    "- Automated tracking ensures reproducibility\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different optimization strategies (grid, random)\n",
    "- Optimize for different metrics (F1, precision, recall)\n",
    "- Implement early stopping for faster optimization\n",
    "- Set up automated hyperparameter tuning pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
